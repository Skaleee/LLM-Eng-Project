{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ff6ad0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš¨ `use_probabilistic_routing` is part of PhimoeModel.forward's signature, but not documented. Make sure to add it to the docstring of the function in /home/tkode/Desktop/transformers/src/transformers/models/phimoe/modeling_phimoe.py.\n",
      "ðŸš¨ `prob_routing_temp` is part of PhimoeModel.forward's signature, but not documented. Make sure to add it to the docstring of the function in /home/tkode/Desktop/transformers/src/transformers/models/phimoe/modeling_phimoe.py.\n",
      "ðŸš¨ `use_probabilistic_routing` is part of PhimoeForCausalLM.forward's signature, but not documented. Make sure to add it to the docstring of the function in /home/tkode/Desktop/transformers/src/transformers/models/phimoe/modeling_phimoe.py.\n",
      "ðŸš¨ `prob_routing_temp` is part of PhimoeForCausalLM.forward's signature, but not documented. Make sure to add it to the docstring of the function in /home/tkode/Desktop/transformers/src/transformers/models/phimoe/modeling_phimoe.py.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/tkode/.cache/huggingface/hub/models--microsoft--Phi-mini-MoE-instruct/snapshots/e8b3fcb41fef5b2e8644cffa99c83924db6a3ed4/config.json\n",
      "Model config PhimoeConfig {\n",
      "  \"architectures\": [\n",
      "    \"PhiMoEForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_slimmoe.PhiMoEConfig\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_slimmoe.PhiMoEForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"expert_dropout\": 0.0,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"input_jitter_noise\": 0.01,\n",
      "  \"intermediate_size\": 960,\n",
      "  \"lm_head_bias\": true,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"phimoe\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_experts_per_tok\": 2,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"num_local_experts\": 16,\n",
      "  \"output_router_logits\": false,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"router_aux_loss_coef\": 0.0,\n",
      "  \"router_jitter_noise\": 0.01,\n",
      "  \"sliding_window\": 2047,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.54.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/tkode/.cache/huggingface/hub/models--microsoft--Phi-mini-MoE-instruct/snapshots/e8b3fcb41fef5b2e8644cffa99c83924db6a3ed4/model.safetensors.index.json\n",
      "Instantiating PhimoeForCausalLM model under default dtype torch.float16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f454204061ca4752a21b3f8422abb8ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing PhimoeForCausalLM.\n",
      "\n",
      "All the weights of PhimoeForCausalLM were initialized from the model checkpoint at microsoft/Phi-mini-MoE-instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use PhimoeForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/tkode/.cache/huggingface/hub/models--microsoft--Phi-mini-MoE-instruct/snapshots/e8b3fcb41fef5b2e8644cffa99c83924db6a3ed4/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": [\n",
      "    32000,\n",
      "    32001,\n",
      "    32007\n",
      "  ],\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n",
      "loading file tokenizer.model from cache at None\n",
      "loading file tokenizer.json from cache at /home/tkode/.cache/huggingface/hub/models--microsoft--Phi-mini-MoE-instruct/snapshots/e8b3fcb41fef5b2e8644cffa99c83924db6a3ed4/tokenizer.json\n",
      "loading file added_tokens.json from cache at /home/tkode/.cache/huggingface/hub/models--microsoft--Phi-mini-MoE-instruct/snapshots/e8b3fcb41fef5b2e8644cffa99c83924db6a3ed4/added_tokens.json\n",
      "loading file special_tokens_map.json from cache at /home/tkode/.cache/huggingface/hub/models--microsoft--Phi-mini-MoE-instruct/snapshots/e8b3fcb41fef5b2e8644cffa99c83924db6a3ed4/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/tkode/.cache/huggingface/hub/models--microsoft--Phi-mini-MoE-instruct/snapshots/e8b3fcb41fef5b2e8644cffa99c83924db6a3ed4/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PhimoeForCausalLM(\n",
       "  (model): PhimoeModel(\n",
       "    (embed_tokens): Embedding(32064, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x PhimoeDecoderLayer(\n",
       "        (self_attn): PhimoeSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "        )\n",
       "        (block_sparse_moe): PhimoeSparseMoeBlock(\n",
       "          (gate): Linear(in_features=4096, out_features=16, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-15): 16 x PhimoeBlockSparseTop2MLP(\n",
       "              (w1): Linear(in_features=4096, out_features=960, bias=False)\n",
       "              (w2): Linear(in_features=960, out_features=4096, bias=False)\n",
       "              (w3): Linear(in_features=4096, out_features=960, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "    (rotary_emb): PhimoeRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32064, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, PhimoeConfig, PhimoeForCausalLM\n",
    "from transformers.utils import logging\n",
    "from accelerate import init_empty_weights, infer_auto_device_map\n",
    "\n",
    "# Optional: Helps debugging\n",
    "logging.set_verbosity_info()\n",
    "device = torch.device(\"cuda:0\")\n",
    "# Model path\n",
    "model_path = \"microsoft/Phi-mini-MoE-instruct\"\n",
    "\n",
    "# Load config\n",
    "config = PhimoeConfig.from_pretrained(model_path)\n",
    "model = PhimoeForCausalLM.from_pretrained(model_path,config=config,\n",
    "                                         device_map={\"\": device},\n",
    "                                         torch_dtype=torch.float16,\n",
    "                                         )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d3368e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 'ids': 'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 21\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     12\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[1;32m     14\u001b[0m         max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;66;03m#return_dict=True\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     )\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Desktop/transformers/src/transformers/tokenization_utils_base.py:3841\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3838\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3839\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3841\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3843\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3845\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3846\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/transformers/src/transformers/tokenization_utils_fast.py:682\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    681\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m [token_ids]\n\u001b[0;32m--> 682\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    684\u001b[0m clean_up_tokenization_spaces \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    685\u001b[0m     clean_up_tokenization_spaces\n\u001b[1;32m    686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    687\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_up_tokenization_spaces\n\u001b[1;32m    688\u001b[0m )\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 'ids': 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "if \"inputs\" in globals():\n",
    "    del inputs\n",
    "\n",
    "prompt = 'I am an AI created in'\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# prob_routing_temp cannot be lower than about 0.0001. \n",
    "# 0.00001 causes cuda assert\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        use_probabilistic_routing=False,\n",
    "        prob_routing_temp=0.001,\n",
    "        #output_router_logits=True,\n",
    "        #return_dict=True\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38a5d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
